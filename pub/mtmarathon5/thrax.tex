\documentclass{pbml}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{multirow}

\begin{document}

\title{Thrax: a Fast Extractor for Synchronous Context-Free Grammars}
\institute{clsp}{Center for Language and Speech Processing, Johns Hopkins University}
\author{firstname=Jonathan, surname=Weese, institute={clsp}, corresponding=yes, email={jonny@cs.jhu.edu}, address={Computational Science and Engineering Building, Room 324 \\ Johns Hopkins University \\ 3400 N. Charles Street, Baltimore, MD 21218, United States}}

\shorttitle{Thrax}
\shortauthor{J. Weese}

\maketitle

\begin{abstract}
We present Thrax, a program that extracts synchronous context-free grammars (SCFGs) for machine translation from aligned parallel corpora. Thrax supports both Hiero-style grammars \cite{Chiang2007} and Syntax-Augmented grammars \cite{samt2006}. It allows two types of concurrency: first, for users with access to a large machine, it supports the native multi-threading of Java; second, it has been reimplemented in Hadoop, an open-source framework for large-scale parallelization. It is designed to allow users to easily add support for other grammars and more feature functions.
\end{abstract}

\section{Introduction}

Modern hierarchical phrase-based machine translation (MT) systems often use a formalism based on synchronous context-free grammars (SCFGs) \cite{Chiang2006}. Formally, a probabilistic SCFG can be considered as a tuple
$$(N,T_\sigma,T_\tau,G,s)$$
where $N$ is a set of non-terminal symbols, $T_\sigma$ and $T_\tau$ are sets of source- and target-side terminal symbols, respectively, $s$ is the start symbol for the grammar, and $G$ is a set of rewrite rules of the form
$$X \to \langle \gamma,\alpha,\sim,\vec{w} \rangle$$
In each rule, $X \in N$ is a non-terminal symbol, and it can be rewritten as a pair of strings $\gamma$ and $\alpha$. We have $\gamma$ as a string over the set $N \cup T_\sigma$ and $\alpha$ over $N \cup T_\tau$. There is a one-to-one correspondence between the non-terminal symbols of each string, which is represented by $\sim$. Finally, $\vec{w}$ is a collection of feature scores associated with each rule.

Given an input sentence, an MT decoder using this SCFG formalism will parse it using the source side of this grammar. Since the grammar is probabilistic, many parses with different weights are possible. This action of parsing automatically creates a target-side forest by considering the target side of each rule application that was used in the parse. This forest represents all possible translation candidates for the input sentence. The decoder may then perform operations on the forest, such as using the Viterbi algorithm to find the one best translation, or extracting the top $k$ translation candidates for some $k$.

In section \ref{sec:extraction}, we describe how such an SCFG can be derived from training data. In section \ref{sec:features}, we describe the feature scores that Thrax can calculate for an extracted grammar. Section \ref{sec:hadoop} describes the design of Thrax's Hadoop components. Section \ref{sec:usage} shows how to use Thrax, and section \ref{sec:performance} presents its speed on various sets of training data.

\section{Grammar extraction}
\label{sec:extraction}
Thrax is able to derive grammars for two types of models, known as Hiero and Syntax-Augmented MT (SAMT).

\subsection{Hiero-style grammar extraction}

Hiero is a simple model for hierarchical phrase-based translation \cite{Chiang2007}. The training data for a Hiero grammar consists of a parallel corpus that has been word-aligned. 

Given a word-aligned sentence pair, as shown in figure \ref{fig:hiero-aligned-pair}, we can enumerate all of the {\em initial phrase pairs} that are consistent with the alignment. An initial phrase pair is a pair of spans, $(i,j)$ on the source sentence and $(a,b)$ on the target side, such that the following hold conditions hold:
\begin{enumerate}
\item for any word $k$ in the span $(i,j)$ on the source side, all the words on the target side that it is aligned to are in the span $(a,b)$;
\item similarly, for any word $c$ in $(a,b)$ on the target side, any words on the source side that $c$ is aligned to must be within the span $(i,j)$;
\item unaligned words are forbidden at the edges of either span.
\end{enumerate}

Once we have enumerated these phrase pairs, we can extract context-free rules in the following way: first, for any phrase pair $(\gamma,\alpha)$, the rule $X \to \langle \gamma ; \alpha \rangle$ can be extracted. Secondly, if we have phrase pairs that are completely contained inside another phrase pair, we can {\em elide} these smaller phrase pairs from the larger rule. Such an example is shown in figure \ref{fig:hiero-hierarchical}.

\subsection{SAMT}

Syntax-Augmented MT describes an algorithm for assigning syntactically-motivated labels to the non-terminal symbols of Hiero rules \cite{samt2006}. That is, the rules have the same shape in both types of grammar --- they are extracted in the same way, from initial phrase pairs --- but in an SAMT grammar different labels are assigned, as opposed to the uniform usage of $X$ in a Hiero grammar.

We wish these non-terminal labels to be syntactically motivated. This requires a slightly different kind of training data from a Hiero extraction. In Hiero, we want simply a word-aligned parallel corpus. In SAMT, we use a word-aligned parallel corpus, where the target corpus sentences have all been parsed by some parser.

With this

\section{Feature functions}
\label{sec:features}

Thrax includes support for extracting several feature functions, and can be easily extended to include more. In this section, we describe the grammar features that this release of Thrax includes, then describe the feature function interface for both the Java-native implementation and Hadoop.

\subsection{Lexical probabilities}

Thrax can calculate the lexical probabilities for a rule in both source-to-target and target-to-source directions. In practice, lexical probability features significantly improve translation performance. In Thrax, we calculate lexical probabilities in the manner of \cite{koehn2003}. First, for each word $w$ on either side of our parallel corpus, we collect $C(w)$, the number of times that word was seen in the corpus. For every ordered pair of words $(x,y)$, where $x$ is a source word and $y$ is a target word, we collect $C(x,y)$, the number of times that $x$ was aligned with $y$ over the whole corpus. Then we can use relative frequencies to estimate the word-level lexical probability as
$$p(y|x) = \frac{C(x,y)}{\sum_{y'}C(x,y')} = \frac{C(x,y)}{C(x)}$$
With these probabilities in hand, we can calculate the lexical probability over a whole rule by taking a sum over all terminal symbols on one side. If $\{y | (x,y) \in A\}$ is the set of all words $y$ that are aligned to $x$ under alignment $A$, we define a rule's lexical probability as
$$p_{\textsc{lex}}(\alpha | \gamma) = \sum_{x \in T(\gamma)}\frac{1}{|\{y | (x,y) \in A\}|}\prod_{y}p(y|x)$$
Similarly, we define $p_{\textsc{lex}}(\gamma | \alpha)$ as a similar sum taken over $T(\alpha)$, the terminal symbols of $\alpha$, where the terms in the product are word-level lexical probabilities for the source words given the target word.

\subsection{Phrasal translation probabilities}
For a rule $X \to \langle \gamma ; \alpha \rangle$ we also estimate the probability the $\gamma$ and $\alpha$ are produced at the same time. That is, we want to estimate the probability that $\alpha$ will be the target-side phrase of the rule, conditioned on $\gamma$ being the source side, and vice-versa. In practice, we estimate these probabilities as relative frequencies: if $C(\alpha)$ is the number of times $\alpha$ was seen as a target-side phrase among the extracted rules, and $C(\gamma,\alpha)$ is the number of times the two phrases co-occurred, we say
$$p_{\textsc{phrase}}(\gamma | \alpha) = \frac{C(\gamma,\alpha)}{\sum_{\gamma'}C(\gamma',\alpha)} = \frac{C(\gamma,\alpha)}{C(\alpha)}$$
and similarly for the phrasal probability $p(\alpha|\gamma)$.

\subsection{Rule probabilities given left-hand symbol}
We also include a relative frequency estimation for the probability that a certain pair of phrases $\langle \gamma ; \alpha \rangle$ will be generated given a left-hand symbol $X$. The intuition is that if a rule can be extracted many times from a training set, it can probably be used very often to explain source and target strings, so we'd like to give it a higher score than some very rare rule. We therefore score it in the obvious way:
$$p_{\textsc{rule}}(\langle \gamma ; \alpha \rangle | X) = \frac{C(X \to \langle \gamma ; \alpha \rangle)}{C(X)}$$
Here, the numerator counts the number of times that the rule has been extracted through the entire corpus, and the denominator shows how often any rule in the corpus has had $X$ as its left-hand side symbol.

\subsection{Feature function interface}

In the native Java implementation, the feature function interface consists of two methods: first, a {\sc note-extraction} method, which is called each time a rule is extracted from a training sentence pair, and second, a {\sc score} method, which calculates the feature score for a given rule. The reason for separating these two methods is straightforward: for some features, the score for a given rule is able to be calculated as soon as the rule is extracted, but for many features this is not the case. For example, if we had a feature counting the number of unaligned words on each rule's source side, this score could be calculated immediately. But other features, such as lexical probability and phrasal probability, require statistics to be calculated over the entire corpus. In such cases, the {\sc note-extraction} method handles accumulating the statistics. After all rules have been extracted, so the entire corpus has been seen, the {\sc score} method can calculate the actual feature scores using the now pre-computed statistics.

\section{Sun Grid Engine architecture}
\label{sec:sge}

\section{Hadoop architecture}
\label{sec:hadoop}

\section{Usage}
\label{sec:usage}

\section{Performance}
\label{sec:performance}

\begin{table}[t]
\caption{Comparison of Thrax extraction to Joshua's built-in extractor and the SAMT extractor of Zollmann and Venugopal.}
\begin{tabular}{l | l | l | r | r | r}
Dataset & Sentences & Grammar & System & Time (sec.) & BLEU \\ \hline
% \multirow{4}{*}{BTEC zh-en} & \multirow{4}{*}{44K} & \multirow{2}{*}{Hiero} & Thrax & 360 & 25 \\
% & & & Joshua & & \\
% & & \multirow{2}{*}{SAMT} & Thrax & & \\
% & & & ZV & & \\ \hline
\multirow{4}{*}{NIST09 ur-en} & \multirow{4}{*}{165K} & \multirow{2}{*}{Hiero} & Thrax & & \\
& & & Joshua & & \\
& & \multirow{2}{*}{SAMT} & Thrax & & \\
& & & ZV & & \\

\end{tabular}
\end{table}

\bibliography{thrax}
\end{document}
